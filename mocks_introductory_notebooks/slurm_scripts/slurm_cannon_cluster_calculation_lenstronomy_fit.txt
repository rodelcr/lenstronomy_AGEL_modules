#!/bin/bash
#SBATCH -n 5 #number of cores
#SBATCH -p hernquist,hernquist_ice,itc_cluster,shared,sapphire
#SBATCH --mem-per-cpu=100GB # Memory per cpu in GB (see also --mem)
#SBATCH --time 00:30:00 # This was set up for a quick PSO, it does not need a lot of time, unlike MCMC
#SBATCH --job-name=PSO_double_source_test_mock1
#SBATCH -o PSO_double_source_test_mock1%A.out
#SBATCH -e PSO_double_source_test_mock1%A.err
#SBATCH --mail-type=END,FAIL               # from "-m ea" (end + abort/fail)
#SBATCH --mail-user=rodrigo.cordova_rosado@cfa.harvard.edu # Set up one's own email to receive information on the state of the run
#
# ----------------Modules------------------------- #
module load python/3.10.13-fasrc01
module load gcc/14.2.0-fasrc01
module load intel/24.2.1-fasrc01
module load mpich/4.2.2-fasrc01 

# ----------------Your Commands------------------- #
#
# ----- One needs to edit the path where all these files are found
cd /n/holystore01/LABS/hernquist_lab/Lab/rcordova/lenstronomy_AGEL_modules/mocks_introductory_notebooks/

echo + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME
echo + NSLOTS = $NSLOTS distributed over:sort $TMPDIR/machines | uniq -c
#
set -euo pipefail

echo "+ $(date) job ${SLURM_JOB_NAME} started in partition=${SLURM_JOB_PARTITION} with jobID=${SLURM_JOB_ID} on ${HOSTNAME}"

source /n/holystore01/LABS/hernquist_lab/Everyone/raglita/grav_lenses/myenv_duncan/bin/activate #----- Luckily Rosa already made an installation 

srun -n $SLURM_NTASKS --mpi=pmix python -u model_fitting_sequence_rc_cluster.py PSO_double_source_test_mock1 

echo "= $(date) job ${SLURM_JOB_NAME} done"

